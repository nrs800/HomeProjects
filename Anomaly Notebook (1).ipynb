{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1359abb9",
   "metadata": {},
   "source": [
    "# Anomaly Detection Code Notebook\n",
    "\n",
    "This code is based on the Anomaly code written by Nathanael Seay\n",
    "\n",
    "1. This code first takes multiple files representing bearing noise and strings them together into one long waveform\n",
    "1. Then builds a NN based on the LSTM-Autoencoder \n",
    "1. Predictions are made and the reconstruction loss is measured\n",
    "1. Any loss greater than the set threshold is flagged as an anomaly\n",
    "1. Anomalys are then traced back to their original files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ffa09",
   "metadata": {},
   "source": [
    "# load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795814e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "# from Anomaly_Shaper import Shaper\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed)\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61017c55",
   "metadata": {},
   "source": [
    "# load files\n",
    "These file locations are on the AWS cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path1 = \"s3://sensor-data-nate/Sensor Data/Bearing_Sensor_Data_pt1\"\n",
    "#path2 = \"s3://sensor-data-nate/Sensor Data/Bearing_Sensor_Data_pt2\"\n",
    "\n",
    "\n",
    "#filenames1 = glob.glob(path1 + \"/*.39\")\n",
    "#filenames2 = glob.glob(path2 + \"/*.39\")\n",
    "#filenames= filenames1+filenames2\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def download_text_files(bucket_name, prefix1='', prefix2='', local_path='./'):\n",
    "    \"\"\"\n",
    "    Download text files from an S3 bucket to a local directory.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: The name of the S3 bucket.\n",
    "    - prefix1: (Optional) The first prefix (folder) within the S3 bucket.\n",
    "    - prefix2: (Optional) The second prefix (folder) within the S3 bucket.\n",
    "    - local_path: (Optional) The local directory where files will be downloaded.\n",
    "\n",
    "    Returns:\n",
    "    - List of downloaded file paths.\n",
    "    \"\"\"\n",
    "    downloaded_files = []\n",
    "\n",
    "    # List objects in the bucket for prefix1\n",
    "    response1 = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix1)\n",
    "    for obj in response1.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.39'):  # Only download text files\n",
    "            local_file_path = os.path.join(local_path, key.split('/')[-1])  # Extract filename\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "            downloaded_files.append(local_file_path)\n",
    "\n",
    "    # List objects in the bucket for prefix2\n",
    "    response2 = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix2)\n",
    "    for obj in response2.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.39'):  # Only download text files\n",
    "            local_file_path = os.path.join(local_path, key.split('/')[-1])  # Extract filename\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "            downloaded_files.append(local_file_path)\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    # Replace 'your-bucket-name' with the actual name of your S3 bucket\n",
    "bucket_name = 'sensor-data-nate'\n",
    "\n",
    "    # Specify the prefix (folder) within the bucket (optional)\n",
    "prefix1 = 'Sensor Data/Bearing_Sensor_Data_pt1/'\n",
    "prefix2 = 'Sensor Data/Bearing_Sensor_Data_pt2/'\n",
    "\n",
    "    # Specify the local directory where files will be downloaded (optional)\n",
    "local_path = './Sensor Data/Bearing_Sensor_Data_pt1/'\n",
    "\n",
    "    # Download text files from the S3 bucket\n",
    "downloaded_files = download_text_files(bucket_name, prefix1, prefix2, local_path)\n",
    "\n",
    "print(\"Downloaded files:\")\n",
    "for file_path in downloaded_files:\n",
    "    print(file_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e742661",
   "metadata": {},
   "source": [
    "# Defining Shaper() function\n",
    "Function stings files into one long waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shaper(bearing, filenames):\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    dfs1 = pd.DataFrame()\n",
    "    dfs2 = pd.DataFrame()\n",
    "    dfs3 = pd.DataFrame()\n",
    "    dfs4 = pd.DataFrame()\n",
    "    \n",
    "    for filename in filenames:\n",
    "        df=pd.read_csv(filename, sep='\\t', header= None)\n",
    "    \n",
    "        #df1 = df.iloc[0]\n",
    "        df1 = df.iloc[:, lambda df: [0]]\n",
    "    \n",
    "        dfs1= pd.concat([dfs1, df1], axis = 1)\n",
    "        \n",
    "        df2 = df.iloc[:, lambda df: [1]]\n",
    "    \n",
    "        dfs2= pd.concat([dfs2, df2], axis = 1)\n",
    "        \n",
    "        df3 = df.iloc[:, lambda df: [2]]\n",
    "    \n",
    "        dfs3= pd.concat([dfs3, df3], axis = 1)\n",
    "        \n",
    "        df4 = df.iloc[:, lambda df: [3]]\n",
    "    \n",
    "        dfs4= pd.concat([dfs4, df4], axis = 1)\n",
    "    \n",
    "    # melt1 = pd.melt(dfs1)\n",
    "    # melt2 = pd.melt(dfs2)\n",
    "    # melt3 = pd.melt(dfs3)\n",
    "    # melt4 = pd.melt(dfs4)\n",
    "    \n",
    "    if (bearing==1):\n",
    "        melt = pd.melt(dfs1)\n",
    "        \n",
    "    if (bearing==2):\n",
    "        melt = pd.melt(dfs2)\n",
    "        \n",
    "    if (bearing==3):\n",
    "        melt = pd.melt(dfs1)\n",
    "            \n",
    "    if (bearing==4):\n",
    "        melt = pd.melt(dfs2)\n",
    "        \n",
    "    melt = melt[\"value\"]\n",
    "        \n",
    "    return melt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e37870",
   "metadata": {},
   "source": [
    "# Calling Shaper() \n",
    "Giving function bearing and filenames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c444b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearing = 4\n",
    "filenames = downloaded_files\n",
    "wave_data = Shaper(bearing, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de661b18",
   "metadata": {},
   "source": [
    "# Plotting waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ce4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wave_data, label='bearing', linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38e7a7",
   "metadata": {},
   "source": [
    "# Make training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9684db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_amount = int(len(wave_data) * 0.8)\n",
    "test_amount = len(wave_data)\n",
    "\n",
    "train = wave_data.iloc[:train_amount]\n",
    "test = wave_data.iloc[train_amount:]\n",
    "\n",
    "print(\"Training dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc36776",
   "metadata": {},
   "source": [
    "# Transform and scale training and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train.values.reshape(-1, 1)) \n",
    "X_test = scaler.transform(test.values.reshape(-1, 1))  \n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0125224",
   "metadata": {},
   "source": [
    "# Make LSTM - Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], 1))  \n",
    "    L1 = LSTM(64, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(16, activation='relu', return_sequences=True)(L1)\n",
    "    L3 = LSTM(4, activation='relu', return_sequences=False)(L2)\n",
    "    L4 = RepeatVector(X.shape[1])(L3)\n",
    "    L5 = LSTM(4, activation='relu', return_sequences=True)(L4)\n",
    "    L6 = LSTM(16, activation='relu', return_sequences=True)(L5)\n",
    "    L7 = LSTM(64, activation='relu', return_sequences=True)(L6)\n",
    "    output = TimeDistributed(Dense(1, activation='relu'))(L7)  \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa2342",
   "metadata": {},
   "source": [
    "# Defining epochs and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484a571",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n",
    "                    validation_split=0.05).history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd40e9e",
   "metadata": {},
   "source": [
    "# Making Predictions based on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8e5f6",
   "metadata": {},
   "source": [
    "# Adding reconstruction loss to 'scored' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afa15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = pd.DataFrame()\n",
    "#Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a1925",
   "metadata": {},
   "source": [
    "# Determining the treshold value based on reconstuction loss distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08640d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = scored['Loss_mae'].min()\n",
    "max_value = scored['Loss_mae'].max()\n",
    "range_ = max_value - min_value\n",
    "range_of=  range_*.98\n",
    "\n",
    "threshold = min_value+ range_of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77fb85",
   "metadata": {},
   "source": [
    "# Making predictions based on test data set\n",
    "This is the remaining data and the set were anomalies are most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed289a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_test)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred)\n",
    "X_pred.index = test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af43809",
   "metadata": {},
   "source": [
    "# Determining that anomalies based on reconstruction loss threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = pd.DataFrame(index=test.index)\n",
    "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[1])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\n",
    "scored['Threshold'] = threshold\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c399ec",
   "metadata": {},
   "source": [
    "# Isolating anomalies and adding an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf886df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback(scored, X_train):\n",
    "    \n",
    "\n",
    "  \n",
    "    import pandas as pd\n",
    "   \n",
    "    \n",
    "    sample_rate= 20480\n",
    "    entry_number= len(scored['Loss_mae'])\n",
    "    print(entry_number)\n",
    "    increment= entry_number/sample_rate\n",
    "   \n",
    "    fileID = []\n",
    "    j = 1\n",
    "    for i in range(0, entry_number):\n",
    "        #if i % increment:  # Check if the index is a multiple of the increment\n",
    "        j=j+1\n",
    "        fileID.append(j)\n",
    "    scored['File_ID'] = fileID\n",
    "    #print(fileID)\n",
    "    ID_scored = scored.loc[scored['Anomaly'] == True]\n",
    "    \n",
    "    ID_scored = ID_scored.drop_duplicates(subset=['File_ID'])\n",
    "   \n",
    "    ID_scored['file_pointer']=(ID_scored['File_ID']+len(X_train))/sample_rate\n",
    "    return ID_scored\n",
    "\n",
    "ID_scored=traceback(scored, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa367c",
   "metadata": {},
   "source": [
    "# Tracing anomalies back to their files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99578862",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pointer=(ID_scored['file_pointer']).astype(int) \n",
    "\n",
    "file_pointer=file_pointer.drop_duplicates()\n",
    "\n",
    "f = open(\"Anomaly_log.txt\", \"a\")\n",
    "\n",
    "for index in file_pointer:\n",
    "    if 0 <= index < len(filenames):\n",
    "        print(f\"File at index {index}: {filenames[index]}\")\n",
    "        f.write(f\"File at index {index}: {filenames[index]}\")\n",
    "    else:\n",
    "        print(f\"Index {index} is out of range.\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81011090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4a259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
